#!/usr/bin/env python3

import scrapy.crawler
import spider.spider
import argparse
import ludocid.ludocid
import ludocid.exceptions
from log.logger import logger

class DoubleInput(Exception):
    def __init__(self):
        super().__init__("both arguments --search and --ludocid cannot be supplied simultaneously")

class NoInput(Exception):
    def __init__(self):
        super().__init__("either the argument --search or --ludocid must be supplied")


# e.g. args.search "Aspria Berlin Ku'damm"
# e.g. ldocid=3435981545910666830

parser = argparse.ArgumentParser()
parser.add_argument("-l", "--ludocid", help="cid of the business whose review are to be scraped", type=int)
parser.add_argument("-s", "--search", help="search term")
parser.add_argument("-o", "--output", help="the CSV file name of the file where the output is stored")
args = parser.parse_args()


if __name__ == "__main__":
    if args.ludocid:
        if args.search:
            logger.error(DoubleInput())
            raise DoubleInput
        else:
            ludocid = args.ludocid
    elif args.search:
        try:
            cid = ludocid.ludocid.cid(args.search)
            logger.debug(f"ludocid={cid}")
        except ludocid.exceptions.NoDriver as NoDriver:
            logger.error(ludocid.exceptions.NoDriver())
            raise NoDriver
        except ludocid.exceptions.BadSearch as BadSearch:
            logger.error(ludocid.exceptions.BadSearch())
            raise BadSearch
    else:
        logger.error(NoInput())
        raise NoInput

    process = scrapy.crawler.CrawlerProcess(settings={"REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7"})
    if args.output:
        process.crawl(spider.spider.ReviewSpider, ludocid=cid, csv_filename=args.output)
    else:
        process.crawl(spider.spider.ReviewSpider, ludocid=cid)
    process.start()